{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on Kaggle: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "is_kaggle = \"KAGGLE_WORKING_DIR\" in os.environ or \"/kaggle\" in os.getcwd()\n",
    "print(\"Running on Kaggle:\", is_kaggle)\n",
    "\n",
    "if is_kaggle:\n",
    "    path = Path(\"/kaggle/input/us-patent-phrase-to-phrase-matching\")\n",
    "    ! pip install -q datasets\n",
    "else:\n",
    "    path = Path(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(path/\"train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good starting point within any Kaggle competition is to check the what our data consists of. To do this we should:\n",
    "\n",
    "1. Print out the data frame\n",
    "2. Read the [Dataset description](https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching/data)\n",
    "3. Call dataframe's `describe()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37d61fd2272659b1</td>\n",
       "      <td>abatement</td>\n",
       "      <td>abatement of pollution</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7b9652b17b68b7a4</td>\n",
       "      <td>abatement</td>\n",
       "      <td>act of abating</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36d72442aefd8232</td>\n",
       "      <td>abatement</td>\n",
       "      <td>active catalyst</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5296b0c19e1ce60e</td>\n",
       "      <td>abatement</td>\n",
       "      <td>eliminating process</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54c1e3b9184cb5b6</td>\n",
       "      <td>abatement</td>\n",
       "      <td>forest region</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36468</th>\n",
       "      <td>8e1386cbefd7f245</td>\n",
       "      <td>wood article</td>\n",
       "      <td>wooden article</td>\n",
       "      <td>B44</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36469</th>\n",
       "      <td>42d9e032d1cd3242</td>\n",
       "      <td>wood article</td>\n",
       "      <td>wooden box</td>\n",
       "      <td>B44</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36470</th>\n",
       "      <td>208654ccb9e14fa3</td>\n",
       "      <td>wood article</td>\n",
       "      <td>wooden handle</td>\n",
       "      <td>B44</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36471</th>\n",
       "      <td>756ec035e694722b</td>\n",
       "      <td>wood article</td>\n",
       "      <td>wooden material</td>\n",
       "      <td>B44</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36472</th>\n",
       "      <td>8d135da0b55b8c88</td>\n",
       "      <td>wood article</td>\n",
       "      <td>wooden substrate</td>\n",
       "      <td>B44</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36473 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id        anchor                  target context  score\n",
       "0      37d61fd2272659b1     abatement  abatement of pollution     A47   0.50\n",
       "1      7b9652b17b68b7a4     abatement          act of abating     A47   0.75\n",
       "2      36d72442aefd8232     abatement         active catalyst     A47   0.25\n",
       "3      5296b0c19e1ce60e     abatement     eliminating process     A47   0.50\n",
       "4      54c1e3b9184cb5b6     abatement           forest region     A47   0.00\n",
       "...                 ...           ...                     ...     ...    ...\n",
       "36468  8e1386cbefd7f245  wood article          wooden article     B44   1.00\n",
       "36469  42d9e032d1cd3242  wood article              wooden box     B44   0.50\n",
       "36470  208654ccb9e14fa3  wood article           wooden handle     B44   0.50\n",
       "36471  756ec035e694722b  wood article         wooden material     B44   0.75\n",
       "36472  8d135da0b55b8c88  wood article        wooden substrate     B44   0.50\n",
       "\n",
       "[36473 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>36473</td>\n",
       "      <td>36473</td>\n",
       "      <td>36473</td>\n",
       "      <td>36473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>36473</td>\n",
       "      <td>733</td>\n",
       "      <td>29340</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>37d61fd2272659b1</td>\n",
       "      <td>component composite coating</td>\n",
       "      <td>composition</td>\n",
       "      <td>H01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>152</td>\n",
       "      <td>24</td>\n",
       "      <td>2186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                       anchor       target context\n",
       "count              36473                        36473        36473   36473\n",
       "unique             36473                          733        29340     106\n",
       "top     37d61fd2272659b1  component composite coating  composition     H01\n",
       "freq                   1                          152           24    2186"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include=\"object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to do a lot with not much unique data. There's a lot of repetition and each entry only has 3-4 words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to create an input column for our NLP model to read that combines categorizes and combines all our text columns in to one string we'll input in to the mode;."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    TEXT1: A47; TEXT2: abatement of pollution; ANC...\n",
       "1    TEXT1: A47; TEXT2: act of abating; ANC1: abate...\n",
       "2    TEXT1: A47; TEXT2: active catalyst; ANC1: abat...\n",
       "3    TEXT1: A47; TEXT2: eliminating process; ANC1: ...\n",
       "4    TEXT1: A47; TEXT2: forest region; ANC1: abatement\n",
       "Name: input, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"input\"] = \"TEXT1: \" + df.context + \"; TEXT2: \" + df.target + \"; ANC1: \" + df.anchor\n",
    "df.input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning models operate on numbers not text. We need to convert our input in to numbers. To do this we need to do two things:\n",
    "1. Tokenization: Split each text up in to tokens\n",
    "2. Numericalization: Convert each token in to a number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers use Datasets for storing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'anchor', 'target', 'context', 'score', 'input'],\n",
       "    num_rows: 36473\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "ds = Dataset.from_pandas(df)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to tokenize & numericalize text varys between different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"microsoft/deberta-v3-small\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens aren't necessarily words as we need to be able to handle text that isn't made up of words such as URLs and we need to limit the size of our vocabularly so less common words will be split up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Declan/micromamba/envs/beginner_nlp/lib/python3.10/site-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['▁This', '▁is', '▁Declan', \"'\", 's', '▁token', 'izer']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.tokenize(\"This is Declan's tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 36473/36473 [00:00<00:00, 62574.84 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenization_func(dataset): return tokenizer(dataset[\"input\"])\n",
    "tokenized_dataset = ds.map(tokenization_func, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This adds a new item to our dataset `input_ids` which converts our text in to numbers that match up with one of our models tokens. This list of tokens is known as the model's vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('TEXT1: A47; TEXT2: abatement of pollution; ANC1: abatement',\n",
       " [1,\n",
       "  54453,\n",
       "  435,\n",
       "  294,\n",
       "  336,\n",
       "  5753,\n",
       "  346,\n",
       "  54453,\n",
       "  445,\n",
       "  294,\n",
       "  47284,\n",
       "  265,\n",
       "  6435,\n",
       "  346,\n",
       "  23702,\n",
       "  435,\n",
       "  294,\n",
       "  47284,\n",
       "  2])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = tokenized_dataset[0]\n",
    "row[\"input\"], row[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁TEXT'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(54453)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers always assume that your labels are within a column named `labels` so we need to rename our score column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.rename_columns({'score':'labels'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice a making a random split for a validation set is [often a bad practice](https://www.fast.ai/2017/11/13/validation-sets/). \n",
    "\n",
    "Note that for Kaggle competitions we use the training data for our validation set. The test data is for our test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'anchor', 'target', 'context', 'labels', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 27354\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'anchor', 'target', 'context', 'labels', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 9119\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_dict = tokenized_dataset.train_test_split(0.25, seed=42)\n",
    "ds_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 36/36 [00:00<00:00, 10342.83 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'anchor', 'target', 'context', 'input', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 36\n",
       "})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(path/\"test.csv\")\n",
    "test_df[\"input\"] = \"TEXT1: \" + test_df.context + \"; TEXT2: \" + test_df.target + \"; ANC1: \" + test_df.anchor\n",
    "test_ds = Dataset.from_pandas(test_df).map(tokenization_func, batched=True)\n",
    "test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "### Why not use the loss?\n",
    "The loss function is used during the training process to quantify the difference between the predicted outputs and their actual targets. \n",
    "\n",
    "Metrics are used to evaluate the performance of the model.\n",
    "\n",
    "The loss generally makes for a bad metric because it:\n",
    "* Often doesn't align with our real world objectives\n",
    "* Can be difficult to interprete to stakeholders\n",
    "* Performs badly in imbalanced datasets where one class significantly outnumbers another\n",
    "* Can't detect overfitting\n",
    "* Cannot optimize for multiple objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Coefficient\n",
    "Choosing a metric [can be difficult ](https://www.fast.ai/posts/2019-09-24-metrics.html). Fortunately for Kaggle exercises a metric is provided for us. \n",
    "\n",
    "We'll be using the Correlation coefficient. It ranges from -1 to 1 for complete negative and positive correlation respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def corr(x,y): return np.corrcoef(x,y)[0][1]\n",
    "def corr_d(eval_pred): return {'pearson': corr(*eval_pred)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The batch-size may need adjusting to fit the GPU we choose to use. If a memory crash occurs during training a lower batch size will be required.\n",
    "\n",
    "Learning rate will also require some trial and error, like batch size the larger the value the faster we'll train but a value too large will result in failure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "learning_rate = 8e-5\n",
    "epoch_count = 4\n",
    "if is_kaggle:\n",
    "    batch_size = 128\n",
    "    fp16 = True\n",
    "else:\n",
    "    batch_size = 32\n",
    "    fp16 = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments below should work fine in most cases, diving in to them isn't necessary at this stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments('outputs', \n",
    "                         learning_rate=learning_rate,\n",
    "                         warmup_ratio=0.1,\n",
    "                         lr_scheduler_type='cosine',\n",
    "                         fp16=fp16,\n",
    "                         evaluation_strategy='epoch',\n",
    "                         per_device_train_batch_size=batch_size,\n",
    "                         per_device_eval_batch_size=batch_size*2,\n",
    "                         num_train_epochs=epoch_count,\n",
    "                         weight_decay=0.01,\n",
    "                         report_to='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a `Trainer` which combines our model and data together. It will spit out a few warnings we can ignore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['pooler.dense.bias', 'classifier.weight', 'pooler.dense.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
    "\n",
    "trainer = Trainer(model, \n",
    "                  args, \n",
    "                  train_dataset=ds_dict['train'], \n",
    "                  eval_dataset=ds_dict['test'],\n",
    "                  tokenizer=tokenizer,\n",
    "                  compute_metrics=corr_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3420 [00:00<?, ?it/s]You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 15%|█▍        | 500/3420 [02:37<14:12,  3.42it/s]Checkpoint destination directory outputs/checkpoint-500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.048, 'learning_rate': 7.948100247760808e-05, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 25%|██▌       | 855/3420 [04:57<23:20,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.026030583307147026, 'eval_pearson': 0.7970794374456811, 'eval_runtime': 24.2528, 'eval_samples_per_second': 375.998, 'eval_steps_per_second': 5.896, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 1000/3420 [05:43<11:23,  3.54it/s] Checkpoint destination directory outputs/checkpoint-1000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0289, 'learning_rate': 7.131321954084034e-05, 'epoch': 1.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 1500/3420 [08:43<09:29,  3.37it/s]  Checkpoint destination directory outputs/checkpoint-1500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0213, 'learning_rate': 5.516578043170591e-05, 'epoch': 1.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                        \n",
      " 50%|█████     | 1710/3420 [41:25<12:44,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.023095745593309402, 'eval_pearson': 0.8198431250046073, 'eval_runtime': 20.805, 'eval_samples_per_second': 438.308, 'eval_steps_per_second': 6.873, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 2000/3420 [42:50<07:43,  3.06it/s]  Checkpoint destination directory outputs/checkpoint-2000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0165, 'learning_rate': 3.5153592976500496e-05, 'epoch': 2.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 2500/3420 [1:06:58<04:21,  3.51it/s]   Checkpoint destination directory outputs/checkpoint-2500 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0132, 'learning_rate': 1.6376432206060943e-05, 'epoch': 2.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      " 75%|███████▌  | 2565/3420 [1:22:58<06:05,  2.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.021096045151352882, 'eval_pearson': 0.8363681798840652, 'eval_runtime': 936.7996, 'eval_samples_per_second': 9.734, 'eval_steps_per_second': 0.153, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 3000/3420 [1:25:04<02:01,  3.47it/s]    Checkpoint destination directory outputs/checkpoint-3000 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0101, 'learning_rate': 3.6193470242834415e-06, 'epoch': 3.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                         \n",
      "100%|██████████| 3420/3420 [1:43:38<00:00,  1.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.02154168114066124, 'eval_pearson': 0.8373890015435218, 'eval_runtime': 20.5034, 'eval_samples_per_second': 444.755, 'eval_steps_per_second': 6.974, 'epoch': 4.0}\n",
      "{'train_runtime': 6218.375, 'train_samples_per_second': 17.596, 'train_steps_per_second': 0.55, 'train_loss': 0.021318977687791077, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3420, training_loss=0.021318977687791077, metrics={'train_runtime': 6218.375, 'train_samples_per_second': 17.596, 'train_steps_per_second': 0.55, 'train_loss': 0.021318977687791077, 'epoch': 4.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions\n",
    "Now we've trained our model we can make some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 332.43it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.47082999],\n",
       "       [ 0.57536417],\n",
       "       [ 0.50354177],\n",
       "       [ 0.26576445],\n",
       "       [-0.02382522],\n",
       "       [ 0.55075079],\n",
       "       [ 0.49101523],\n",
       "       [-0.02531304],\n",
       "       [ 0.29687223],\n",
       "       [ 1.09018052],\n",
       "       [ 0.22743776],\n",
       "       [ 0.20256121],\n",
       "       [ 0.78881073],\n",
       "       [ 0.99500763],\n",
       "       [ 0.7485072 ],\n",
       "       [ 0.32892826],\n",
       "       [ 0.28195688],\n",
       "       [-0.0210176 ],\n",
       "       [ 0.60453236],\n",
       "       [ 0.36301944],\n",
       "       [ 0.41035315],\n",
       "       [ 0.26777557],\n",
       "       [ 0.07301307],\n",
       "       [ 0.18113427],\n",
       "       [ 0.5371725 ],\n",
       "       [-0.02387112],\n",
       "       [-0.01914458],\n",
       "       [-0.0300971 ],\n",
       "       [-0.01788442],\n",
       "       [ 0.55236644],\n",
       "       [ 0.3317053 ],\n",
       "       [ 0.00218474],\n",
       "       [ 0.67597705],\n",
       "       [ 0.52917236],\n",
       "       [ 0.44183591],\n",
       "       [ 0.22106177]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = trainer.predict(test_ds).predictions.astype(float)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of these predictions are outside the range of scores we can have (0-1) so we should trim them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.47082999],\n",
       "       [0.57536417],\n",
       "       [0.50354177],\n",
       "       [0.26576445],\n",
       "       [0.        ],\n",
       "       [0.55075079],\n",
       "       [0.49101523],\n",
       "       [0.        ],\n",
       "       [0.29687223],\n",
       "       [1.        ],\n",
       "       [0.22743776],\n",
       "       [0.20256121],\n",
       "       [0.78881073],\n",
       "       [0.99500763],\n",
       "       [0.7485072 ],\n",
       "       [0.32892826],\n",
       "       [0.28195688],\n",
       "       [0.        ],\n",
       "       [0.60453236],\n",
       "       [0.36301944],\n",
       "       [0.41035315],\n",
       "       [0.26777557],\n",
       "       [0.07301307],\n",
       "       [0.18113427],\n",
       "       [0.5371725 ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.        ],\n",
       "       [0.55236644],\n",
       "       [0.3317053 ],\n",
       "       [0.00218474],\n",
       "       [0.67597705],\n",
       "       [0.52917236],\n",
       "       [0.44183591],\n",
       "       [0.22106177]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = np.clip(predictions, 0, 1)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 276.85ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1022"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = Dataset.from_dict({\n",
    "    'id' : test_ds['id'],\n",
    "    'score' : predictions\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
